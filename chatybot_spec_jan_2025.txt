# ChatyBot Specification

## Overview
ChatyBot is a command-line interface for interacting with various AI language models through the OpenAI API. It provides a flexible environment for sending prompts, managing files, and executing scripts.

## Core Functionality


### 2. Chat Interface
- Interactive command-line prompt
- Multi-line input mode
- Input history with tab completion
- Streaming or batch response modes

### 3. File Management
- Five file banks for persistent content storage - like registers for text for LLM input
- File text loading and display capabilities
- filebank1..5 - chatybot can handle 5 corpus of text loaded from files.
- /save command - save last llm chat completion in buffer to a file. IMPORTANT there is no ability to save portions to different files.  only one output is supported
- chat completion does not support multiple-file output. has to be saved as one unit 
- important only one file can be creaed at a time with save - no spliting of text is allowed.  No appendiing
- /filebank and /file can only load text from file.  Important it cannot save text string, nor create files ahead
- a filebank has to loaded from disk - /filebank1 "readme.txt".  it stores it in a string in memory.  you can refer 
  to it later as {filebank1} in text sent to LLM

### 4. Scripting Engine
- Execute sequences of commands from files
- Variable substitution
- Conditional execution
- Delay/wait commands
- there is no way to execute OS commands or shell commands - other than saving and loading files into filebank

## Command Specification

### Standard Commands (prefixed with `/`)
| Command | Parameters | Description |
|---------|------------|-------------|
| `/help` | - | Show available commands |
| `/prompt` | `<file>` | Load LLM prompt text from file |
| `/file` | `<path>` | Load file into buffer that is permenent context for LLM call  |
| `/showfile` | `[all]` | Display file buffer content |
| `/clearfile` | - | Clear file buffer in memory, no effect on disk|
| `/filebank{N}` | `<file>` or `clear` or `show [all]` | Manage file banks (1-5).  load text from file into buffer |
| `/model` | `[alias]` | Show current model or switch models |
| `/listmodels` | - | List available models |
| `/logging` | `<start|end>` | Start/stop logging |
| `/save` | `<file>` | Save last chat response to file |
| `/codeonly` | - | Enable code-only mode |
| `/codeoff` | - | Disable code-only mode |
| `/multiline` | - | Toggle multi-line input mode |
| `/system` | `[message]` | Set/show system prompt for LLM completion |
| `/temp` | `[value]` | Set/show temperature (0.0-2.0) |
| `/maxtokens` | `[value]` | Set/show max tokens |
| `/stream` | - | Toggle streaming responses |
| `/script` | `<file>` | Execute a chatdslscript file |
| `/quit` | - | Exit program |

### Scripting Commands
important: only one line string are supported by parser
| Command | Parameters | Description |
|---------|------------|-------------|
| `set` | `<name> = <value>` | Define a variable |
| `${name}` | - | Reference a variable |
| `if` | `<condition> then <command>` | Conditional execution |
| `wait` | `<seconds>` | Pause execution |
| `#` | `<comment>` | Comment line |

## Execution Flow

1. **Initialization**

2. **Main Loop**
   - Display prompt and wait for user input
   - Process escape commands (starting with `/`)
   - For regular input, send to AI model via `chat_completion()`
   - Handle responses (streaming or batch)
   - Log interactions if enabled

3. **Command Processing**
   - Escape commands are routed to `handle_escape_command()`
   - Script commands are processed by `execute_script_command()`
   - Regular input is sent to the AI model
   - there is no way to save arbitrary text to file, only text from LLM completion chat

4. **Script Execution**
   - script uses special domain specific language 
   - Load script file and remove comments
   - Split commands by newlines or semicolons
   - Process each command sequentially
   - Handle variables, conditionals, and delays
   - set variable command does not support multiline strings - consider using /multiline
   - a command must start on line1.  avoid putting /commands in variable.  use that for filenames or other data:
   - a line with no /escape command or variable  will be sent to the the LLM of the last /model command 
   - multiline to be sent to LLM must be inline like this, important to disable it after sequence with another /multiline - boolean toggle
     /multiline
     Please give me details on pears
     1.  varieties
     2.  countries of origin of each variety.
     ;;
     /multiline
   - questions or asks should be inline and not be in a file
   - there is no way to save artibrary text to a file, only LLM command completions can be saved
   - there are no subroutines, no loops, repeat commands if necessary
   - all outputs from chat completion have to be saved to files, for later use
   - favor filebank1, filebank2... filebank5 are like file memory buffers, and serve as special file text variable.
   - do not use /file
   - there are no conventions for filenames created, but prefer .txt
   - any commentary should be commented with line 1 being a "#"


## Key Features

1. **Multi-Model Support**
   - Configure multiple AI models in TOML file
   - Switch between models during runtime

2. **Prompt Engineering**
   - File buffer for context
   - Prompt buffer for structured prompts
   - File banks for text data on content

3. **Response Control**
   - Code-only mode
   - Temperature and max tokens settings
   - Streaming vs batch responses

4. **Scripting Capabilities**
   - Automate complex workflows
   - Variable substitution
   - Conditional logic
   - Delay execution

